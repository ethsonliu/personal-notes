## 目录

- [TCP 何时关闭连接](#TCP-何时关闭连接)
- [SO_KEEPALIVE VS 应用层心跳包机制](#SO_KEEPALIVE-VS-应用层心跳包机制)
- [TCP Keepalive 工作原理](#TCP-Keepalive-工作原理)
- [应用层心跳和 TCP Keepalive](#应用层心跳和-TCP-Keepalive)
- [多线程读写同一套接字](#多线程读写同一套接字)
- [整型的大小端转换](#整型的大小端转换)
- [为什么 TCP 协议握手包序号不从 0 开始](#为什么-TCP-协议握手包序号不从-0-开始)
- [TCP RST 产生的几种情况](#TCP-RST-产生的几种情况)
- [TCP 如何清除大量的 TIME_WAIT](TCP-如何清除大量的-TIME_WAIT)
- [Nagle 算法和延迟确认](#Nagle-算法和延迟确认)
- [出现大量 CLOSE_WAIT](#出现大量-CLOSE_WAIT)
- [为什么 select 函数的第一个参数总应该是 fdmax+1 poll 和 epoll 不需要 +1](#为什么-select-函数的第一个参数总应该是-fdmax+1-poll-和-epoll-不需要-+1)
- [select poll epoll 原理](#select-poll-epoll-原理)
- [linux 函数 write 返回了代表什么](#linux-函数-write-返回了代表什么)
- [select/poll/epoll(水平/边缘)与所监听的描述符阻塞与否之间的关系？](#select/poll/epoll(水平/边缘)与所监听的描述符阻塞与否之间的关系？)
- [SO_REUSEADDR 和 SO_REUSEPORT](#SO_REUSEADDR-和-SO_REUSEPORT)
- [服务端最多多少并发](#服务端最多多少并发)
- [epoll_ctl 和 epoll_wait 可以并行么](#epoll_ctl-和-epoll_wait-可以并行么)
- [UDP 一个包最大可以发送多少个字节](#UDP-一个包最大可以发送多少个字节)
- [IP 头部中的分片偏移量](#IP-头部中的分片偏移量)

## TCP 何时关闭连接

shutdown(both) 还需要 close，不然你就泄漏了一个或几个 handle 或 fd 以及相关资源。

read 返回 0 表示你收到了对方发来的 fin，这可能是对方调用 shutdown(send)，也可能是对方调用了 close()。从 tcp 协议本身来说，你是无法知道对方到底是调用了 close() 还是调用了 shutdown(send) 的，os 的 tcp 协议栈也不知道。因此此时是否要 close 取决于你的应用。通常来说如果对方调用的是 close，那么你也可以 close。否则你不能 close，例如对方发送一个包给你并 shutdown(write)，然后调用 recv，这时候你还可以返回一个或多个包，连接此时处于半关闭状态，可以一直持续。

这么做的客户端不多（connect -> send -> shtudown(send) -> recv()），但的确有，而且是完全合法的。如果通讯双方都是你自己的代码，那么你知道是哪种情况。如果你不能了解对方的代码，甚至你是个 proxy（代理），两边代码你都不了解，那么通常来说你不能 close。

很多 server/proxy 的实现为当 read 返回 0 就 close，这种实现是错误的，这个实现无法兼容刚才我说的那种情况。

对于 proxy 来说，正确的做法是透传双方的行为。因此，当你 read(client_side_socket) 返回 0 时，你应该对另外一端调用 shutdown(server_side_socket, send)，这样服务器就会 read 返回 0，你透明的传递了这个行为。那么作为 proxy，你什么时候才能 close 呢？client_socket 和 server_socket 上 read 都返回了 0，或者有任何一方返回了 -1 时你可以 close。当然你也可以考虑设置一个超时时间，如果线路上超过 5 分钟没有数据你就断开，但这是另一个维度的问题。

关于 close，要注意的是默认情况下它是一个异步的过程。作为 proxy 来说，如果你想避免大量 close_wait，那么你可以在 close 之前 shutdown，然后启动一个 5s 的 delaytask，在 delaytask 里设置超时时间为 0 的 so_linger，然后 close，对 socket 进行 hard close。这时候 close 是同步的，如果此时不能优雅关闭，那么系统会立刻强制关闭。如果你在 2 处 close 同一个 socket，这是高危行为。因为 2 次 close 之间很可能会有一个新 socket 产生并且值和你第一次 close 的那个一样。你第二次 close 就会错误的 close 了一个不属于你的 socket。这种错误非常难查，因此绝对不要干这种事。

参考：

- <https://www.zhihu.com/question/48871684/answer/113135138>

## SO_KEEPALIVE VS 应用层心跳包机制

TCP Keepalive 和应用层 HeartBeat 优缺点，

**TCP 协议的 Keepalive**

优点：

1. 系统内核完全替上层应用自动给做好了，内核层面计时器相比上层应用，更为高效。上层应用只需要处理数据收发、连接异常通知即可。
2. 使用起来简单，减少了应用层代码的复杂度。也会更节省流量，因为应用层的数据传输到 TCP 协议层时都会被加上额外的包头包尾。由 TCP 协议提供的检活，其发的探测包，理论上实现的会更精妙，耗费更少的流量。

缺点:

1. keepalive 只能检测连接存活，而不能检测连接可用，比如某台服务器因为某些原因导致负载超高，CPU 满了,无法响应任何业务请求，但是使用 TCP 探针则仍旧能够确定连接状态，这就是典型的连接活着但业务提供方已死的状态。对客户端而言，这时的最好选择就是断线后重新连接其他服务器，而不是一直认为当前服务器是可用状态，一直向当前服务器发送些必然会失败的请求。
2. 如果 tcp 连接的另一端突然掉线，这个时候我们并不知道网络已经关闭。而此时，如果有发送数据失败，tcp 会自动进行重传。重传包的优先级高于 keepalive 的包，那就意味着，我们的 keepalive 总是不能发送出去。 而此时，我们也并不知道该连接已经出错而中断。在较长时间的重传失败之后，我们才会知道。

**应用层 HeartBeat**

优点:

1. 有着更大的灵活性，可以控制检测时机，间隔和处理流程，甚至可以在心跳包上附带额外信息，最重要的是可以做到没有上面所说的缺点，不光可以检测连接存在，还可以检测连接可用。
2. 通用，应用层的心跳不依赖协议。如果有一天不用 TCP 要改为 UDP 了，协议层不提供心跳机制了，但是你应用层的心跳依旧是通用的，可能只需要做少许改动就可以继续使用。

缺点:

需要自己实现，增加开发工作量，由于应用特定的网络框架，还可能增加代码结构的复杂度，应用层心跳的流量消耗还是更大的，毕竟这本质上还是个普通的数据包。

## TCP Keepalive 工作原理

当一个 TCP 连接建立之后，启用 TCP Keepalive 的一端便会启动一个计时器，当这个计时器数值到达 0 之后（也就是经过 tcp_keep-alive_time 时间后，这个参数之后会讲到），一个 TCP 探测包便会被发出。这个 TCP 探测包是一个纯 ACK 包（规范建议，不应该包含任何数据，但也可以包含 1 个无意义的字节，比如 0x0。），其 Seq 号与上一个包是重复的，所以其实探测保活报文不在窗口控制范围内。

如果一个给定的连接在两小时内（默认时长）没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下 4 个状态之一：

1. 客户主机依然正常运行，并从服务器可达。客户的 TCP 响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。

2. 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的 TCP 都没有响应。服务端将不能收到对探测的响应，并在 75 秒后超时。服务器总共发送 10 个这样的探测 ，每个间隔 75 秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。

3. 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。

4. 客户机正常运行，但是服务器不可达，这种情况与 2 类似，TCP 能发现的就是没有收到探测的响应。

对于 linux 内核来说，应用程序若想使用 TCP Keepalive，需要设置 SO_KEEPALIVE 套接字选项才能生效。

有三个重要的参数：

1. tcp_keepalive_time，在 TCP 保活打开的情况下，最后一次正常数据交换到 TCP 发送保活探测包的间隔，即允许的持续空闲时长，默认值为 7200s（2h），即 TCP_KEEPIDLE。

2. tcp_keepalive_probes 在 tcp_keepalive_time 之后，没有接收到对方确认，继续发送保活探测包次数，默认值为 9 次，即 TCP_KEEPCNT。

3. tcp_keepalive_intvl，在 tcp_keepalive_time 之后，没有接收到对方确认，继续发送保活探测包的发送频率，默认值为 75s，即 TCP_KEEPINTVL。

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>

int main()
{
   int s;
   int optval;
   socklen_t optlen = sizeof(optval);

   /* 创建套接字 */
   if ((s = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP)) < 0) {
      perror("socket()");
      exit(EXIT_FAILURE);
   }

   /* 开启 keepalive */
   optval = 1;
   optlen = sizeof(optval);
   if (setsockopt(s, SOL_SOCKET, SO_KEEPALIVE, &optval, optlen) < 0) {
      perror("setsockopt()");
      close(s);
      exit(EXIT_FAILURE);
   }
   
   optval = 1000;
   optlen = sizeof(optval);
   setsockopt(s, SOL_TCP, TCP_KEEPIDLE, &optval, optlen);
   
   optval = 10;
   optlen = sizeof(optval);
   setsockopt(s, SOL_TCP, TCP_KEEPINTVL, &optval, optlen);
   
   optval = 10;
   optlen = sizeof(optval);
   setsockopt(s, SOL_TCP, TCP_KEEPCNT, &optval, optlen);
   
   return 0;
}

```

## 应用层心跳和 TCP Keepalive

首先，SO_KEEPALIVE 是实现在服务器，客户端被动响应，缺省超时时间为 120 分钟，这是 RFC 协议标准规范。

SO_KEEPALIVE 是实现在 TCP 协议栈（四层），应用层的心跳实现在第七层，本质没有任何区别，但应用层需要自己来定义心跳包格式。

之所以实现在服务器端，是因为与客户端相比，服务器的寿命更长，因为服务器需要不间断地提供服务，而客户端可能由于用户下班而合上电脑（TCP 没有来得及发送 FIN 关闭连接），这样的话，服务器端就会有很多不可用的 TCP 连接（established)，这样的连接依然会占用服务器内存资源，于是就设计这个 keepalive 来检测客户端是否可用，如果几次重传 keepalive ，客户端没有相应，删除连接，释放资源。需要指出的是，超时时间是指 TCP 连接没有任何数据、控制字传输的时间，如果有任何数据传输，会刷新定时器，重新走表。

TCP 协议中的 SO_KEEPALIVE 有几个致命的缺陷：

1. keepalive 只能检测连接是否存活，不能检测连接是否可用。比如服务器因为负载过高导致无法响应请求但是连接仍然存在，此时 keepalive 无法判断连接是否可用。
2. 如果 TCP 连接中的另一方因为停电突然断网，我们并不知道连接断开，此时发送数据失败会进行重传，由于重传包的优先级要高于 keepalive 的数据包，因此 keepalive 的数据包无法发送出去。只有在长时间的重传失败之后我们才能判断此连接断开了。

## 多线程读写同一套接字

这里有三种情况，

1. 两个线程，一个读，一个写；
2. 两个线程，一个读，另一个也是读；
3. 两个线程，一个写，另一个也是写。

下面分别分析。

### 两个线程，一读一写

这是没有问题的。

### 两个线程，都是读（或者都是写）

不可以！

即使加锁也是不行的，因为会存在 short write/read 的情况，也就是，如果实际发送值小于请求值（short write），此时该线程返回，按照一般的设计逻辑会采用循环 send 直到发送值总和等于请求值，那么这样一来，该线程后续发送的数据也有可能会与其他线程的数据交错。

解决方案：

1. 改成消息队列模型，多个线程把数据发送到队列中（队列用 mutex 等保证线程安全），然后仅由一个线程负责取出消息队列中的消息，发送给对端。
2. 每个线程都有自己的 socket。

参考：

1. [socket套接字在多线程发送数据时要加锁吗？- 知乎](https://www.zhihu.com/question/56899596)

## 整型的大小端转换

```c++
uint8_t buf[1024];
buf[0] = (len >> 24) & 0xFF;
buf[1] = (len >> 16) & 0xFF;
buf[2] = (len >> 8)  & 0xFF;
buf[3] = (len)       & 0xFF;
```

```c++
uint8_t buf[1024];
uint32_t n = ((uint32_t)buf[1] << 24) | ((uint32_t)buf[2] << 16) | ((uint32_t)buf[3] << 8) | ((uint32_t)buf[4]);
```

## 为什么 TCP 协议握手包序号不从 0 开始

为什么要随机产生序号，而不直接从 0 开始？

1. 预防 TCP 序列预测攻击.详细请参考 [TCP sequence prediction attack](https://en.wikipedia.org/wiki/TCP_sequence_prediction_attack)
2. 尽可能的避免数据混淆。如果 C/S 断开后，会有可能某些数据仍然滞留在网络中。当 C/S 再次连接，这些延迟数据在建立新连接之后才到达 Server，如果都是从 0 开始会使得新连接与旧连接数据包重叠几率大大增加，造成数据混淆。

其实应该说，不能写死。0 不可以，1 也不可以，也就是说 ISN（初始序列号）只要写死了，就有可能出现上面的问题。

## TCP RST 产生的几种情况

- <https://zhuanlan.zhihu.com/p/30791159>

## TCP 如何清除大量的 TIME_WAIT

- <https://www.unixso.com/Linux/TIME_WAIT.html>

## Nagle 算法和延迟确认

- <https://segmentfault.com/a/1190000022929052>

## 出现大量 CLOSE_WAIT

服务器出现大量的 CLOSE_WAIT 后，服务无法继续正常服务，socket 资源被耗尽。因为 Linux 分配给一个用户的文件句柄是有限的，而 TIME_WAIT 和 CLOSE_WAIT 两种状态如果一直被保持，则文件句柄也就不能 close，导致句柄资源达到上线，接着就会出现大量 Too Many Open Files 错误。

如果出现大量 CLOSE_WAIT，首先需要考虑程序是否有 BUG，是否有 socket 泄露，比如遗漏 `close`。

**案例：**

今天在运行服务器的时候发现一个问题，问题的表现是客户端一直在请求，但是返回给客户端的信息是异常，服务端压根没有收到请求，查看了一下配置信息没有错误，首先查看了一下是不是服务器的连接已经满了，打开 netstat 命令发现服务器的连接有大量的 CLOSE_WAIT 状态的 socket，没怎么遇到这个问题，开始还真有段懵了，第一反应就是是不是客户端的问题（是不是出问题的第一反应都是别人的问题），但是马上补充了一下 socket 状态机的知识，发现这个状态是由于客户端关闭了 socket 连接，发送了 FIN 报文，服务端也发送了 ACK 报文，此时客户端处于 FIN_WAIT_2 状态，服务端处于 CLOSE_WAIT 状态。

可以看出，出现问题的原因是由于我这边没有发送第二个 FIN 报文导致的，分明是我的问题啊，为什么服务器没有发送 FIN 报文呢？我的服务器使用的是嵌入式的 jetty，连接管理应该都是它帮我管理的，重启了一下服务器发现服务器的 CLOSE_WAIT 开始的时候没有出现，之后逐渐的上升，貌似随着请求的数量逐渐增长的，而我这边的日志也非常奇怪，我会在收到请求的时候打印日志，然后在执行完毕的时候输出一个 accesslog 信息，发现日志中有入口的请求日志，但是 accessLog 没有增长，于是单步调试了一下，发现了问题：一个 servlet 的执行走到主流程就走不下去了，阻塞在数据库访问那一步上，具体表现就是获取不到数据库连接！

查看了一下代码，发现原来是自己创建连接，执行 sql，完成之后没有关闭连接，OMG，这么愚蠢的错误。

好了，既然问题能够解决了，现在回头来思考一下问题产生的具体步骤：首先，我这边的大部分请求都需要查询数据库，我的数据库连接池设置的最大连接数是 100，所以每一个请求创建了一个连接，等到 100 个请求就把连接池占满了，但是处理 servlet 的那个线程并没有释放这个连接，于是接下来的请求再去创建数据库连接的时候就会一直阻塞在那里，这里我所用的是 DBCP 作为连接池的，它的实现好像是使用 apache 的 objectPool 来实现的，如果没有可用的连接对象会导致线程等待，好了，servlet 由于得不到数据库连接而阻塞了，这个客户端的请求就一直等待，客户端使用 httpclient 设置了 5s 的请求超时时间，那么超时之后就会抛出异常，关闭连接，关闭连接导致客户端发送了 FIN 报文，我这边的 TCP/IP 返回了 ACK 报文，但是由于处理请求的线程还处于阻塞的状态，所以当前的连接状态时 CLOSE_WAIT。

通常来说，CLOSE_WAIT 在服务器停留的时间很短，且只会发生在被动关闭连接的一端。除非 Kill 掉进程，否则它是不会消失的，意味着一直占用资源。如果发现有大量的 CLOSE_WAIT，那就是被动关闭的一方没有及时发送 FIN，一般来说有以下几种可能：

- 没有显式关闭 Socket 连接，或者死循环导致关闭连接的代码没有执行到，即 FIN 包没有发出，导致 CLOSE_WAIT 不断累积。
- 响应过慢 / 超时设置过小：双方连接不稳定，一方 Timeout，另外一方还在处理逻辑，导致 Close 被延后。

## 为什么 select 函数的第一个参数总应该是 fdmax+1 poll 和 epoll 不需要 +1

先说明一下， 在 Windows 中， 并不要求 select 函数的第一个参数总应该是 fdmax + 1（在 Windows 下， 给定 -1 就行）， 那 linux 中为什么又是呢？

这就涉及到 linux select 第一个参数的函数： 待测试的描述集的总个数。但要注意，待测试的描述集总是从 0，1，2...开始的。所以，假如你要检测的描述符为 8，9，10，那么系统实际也要监测 0，1，2，3，4，5，6, 7，此时真正待测试的描述符的个数为 11 个， 也就是 max（8， 9， 10） + 1。

有两点要注意：

1. 如果你要检测描述符 8, 9, 10, 但是你把 select 的第一个参数定为 8， 实际上只检测 0 到 7， 所以 select 不会感知到 8, 9, 10 描述符的变化。
2. 如果你要检测描述符 8, 9, 10, 且你把 select 的第一个参数定为 11， 实际上会检测 0-10，但是，如果你不把描述如 0 set 到描述符中， 那么 select 也不会感知到 0 描述符的变化。

## select poll epoll 原理

**select**

```c
typedef long int __fd_mask;


/* It's easier to assume 8-bit bytes than to get CHAR_BIT. */
#define __NFDBITS (8 * (int) sizeof (__fd_mask))
#define __FDELT(d) ((d) / __NFDBITS)
#define __FDMASK(d) ((__fd_mask) 1 << ((d) % __NFDBITS))

/* fd_set for select and pselect. */
typedef struct
  {
    /* XPG4.2 requires this member name. Otherwise avoid the name
       from the global namespace. */
#ifdef __USE_XOPEN
    __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];
# define __FDS_BITS(set) ((set)->fds_bits)
#else
    __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS];
# define __FDS_BITS(set) ((set)->__fds_bits)
#endif
  } fd_set;

/* Maximum number of file descriptors in `fd_set'. */
#define FD_SETSIZE __FD_SETSIZE   //__FD_SETSIZE等于1024

/* Access macros for `fd_set'.  */
#define FD_SET(fd, fdsetp)      __FD_SET (fd, fdsetp)
#define FD_CLR(fd, fdsetp)      __FD_CLR (fd, fdsetp)
#define FD_ISSET(fd, fdsetp)    __FD_ISSET (fd, fdsetp)
#define FD_ZERO(fdsetp)         __FD_ZERO (fdsetp)
```

从上面的代码中简化下来，其实可以改成，

```c
typedef struct{
    long int fds_bits[32];
}fd_set;
```

其实 fd_set 就是一个 long int 类型的数组。因为每一位可以代表一个文件描述符。所以 fd_set 最多表示 1024 个文件描述符！

实现原理如下，

我们都知道支持阻塞操作的设备驱动通常会实现一组自身的等待队列（如读/写等待队列），用于支持用户层所需的 BLOCK 或 NONBLOCK 操作。当应用程序通过设备驱动访问该设备时（默认为 BLOCK 操作），若该设备当前没有数据可读或写，则将该用户进程插入到该设备驱动对应的读/写等待队列让其睡眠一段时间，等到有数据可读/写时再将该进程唤醒。

select 就是巧妙的利用等待队列机制让用户进程适当在没有资源可读/写时睡眠，有资源可读/写时唤醒。下面我们看看 select 睡眠的详细过程。

select 会循环遍历它所监测的 fd_set 内的所有文件描述符对应的驱动程序的 poll 函数。驱动程序提供的 poll 函数首先会将调用 select 的用户进程插入到该设备驱动对应资源的等待队列（如读/写等待队列），然后返回一个 bitmask 告诉 select 当前资源哪些可用。当 select 循环遍历完所有 fd_set 内指定的文件描述符对应的 poll 函数后，如果没有一个资源可用（即没有一个文件可供操作），则 select 让该进程睡眠，一直等到有资源可用为止，进程被唤醒（或者 timeout）继续往下执行。 

参考：

- <https://blog.csdn.net/Zorro721/article/details/107565000>
- <https://blog.csdn.net/pyf09/article/details/94480557>

**poll**

调用过程和 select 类似，时间复杂度也是 O(n)。

其和 select 不同的地方是：采用链表的方式替换原有 fd_set 数据结构，而使其没有连接数的限制。

**epoll**

epoll 使用了一个文件描述符管理了多个文件描述符。epoll 的设计与 select 完全不同，epoll 通过在 Linux 内核申请一个简易的文件系统（文件一般使用什么数据结构实现？B+树），调用分为 3 个部分：

1. 调用 epoll_create 建立了一个 epoll 对象（在 epoll 文件系统中为这个句柄对象分配资源）
2. 调用epoll_ctl 向 epoll 对象添加要监听的套接字
3. 调用epoll_wait 收集发生事件的连接

其它的参考 <https://github.com/ethsonliu/personal-notes/blob/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linux%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/13%20-%20epoll%20%E5%87%BD%E6%95%B0%20%26%20%E4%B8%8E%20select%20%E5%92%8C%20poll%20%E7%9A%84%E6%AF%94%E8%BE%83.md#epoll-%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6>

## linux 函数 write 返回了代表什么

Note that write() successfully returning only means that the bytes are buffered to be send. It may even return fewer bytes than your message contains, indicating the buffer is currently full and you need to try sending the remaining bytes later.

TODO：write 返回 EAGAIN 说明什么？

## select/poll/epoll(水平/边缘)与所监听的描述符阻塞与否之间的关系？

select，poll，epoll LT（可以认为是快速的 poll）都是 LT 的，所以文件描述符可以为阻塞的（当然也可以为非阻塞的）。epoll ET 文件描述符必须是非阻塞的。

epoll 的帮助中提到可以支持阻塞和非阻塞两种，其中特别提到 ET 只能配合非阻塞使用。epoll 的常见使用方式都是非阻塞的，相信大家没有在实际应用中碰见 epoll+阻塞模式 socket。下面从理论上分析 epoll 与阻塞模式 socket 的使用 epoll 的 ET 模式：

ET 模式要求对 read 和 write 的调用必须不断调用，直到返回 EAGAIN，只有这样，下一次 epoll_wait 才有可能返回 EPOLLIN 或 EPOLLOUT。当 socket 是阻塞方式的，永远也不可能返回 EAGAIN，只会阻塞在 read/wite 调用上。

epoll 的 LT 模式：LT 模式能够支持阻塞模式的 socket，在阻塞模式下，当有数据到达，epoll_wait 返回 EPOLLIN 事件，此时的处理中调用 read 读取数据，请注意，第一次调用 read，可以保证 socket 里面有数据(EPOLLIN 事件说明有数据)，read 不会阻塞。第二次调用，socket 里面有没有数据是不确定的，要是贸然调用，read 可能就阻塞了，因此不能进行第二次调用，必须等待 epoll_wait 再次返回 EPOLLIN 才能再次 read。因此 LT 模式下的阻塞 socket 使用就必须 read/write 一次就转到 epoll_wait，这对于网络流量较大的应用效率是相当低的，而且一不小心就会阻塞在某个 socket 上，因此 epoll 的 LT+阻塞式socket 几乎不出现在实际应用中。

参考 https://www.zhihu.com/question/25266987 和 https://stackoverflow.com/questions/26269448/why-is-non-blocking-sockets-recommended-in-epoll

## SO_REUSEADDR 和 SO_REUSEPORT

https://zhuanlan.zhihu.com/p/35367402

SO_REUSEADDR 可以用在客户端么？

答：不可以。如果客户端直接 CLOSED，然后又再次向服务器发起一个新连接，谁也不能保证新发起的连接和刚关闭的连接的端口号是不同的，有可能新、老连接的端口号就是一样的。假设新、老连接端口号一致，若老连接的一些数据仍滞留在网络中，这些滞留数据在新连接建立后才到达服务器，鉴于前后端口号一致，TCP 协议就默认这些数据属于新连接，于是数据就这样乱成一锅粥了。

## TODO TCP 协议中若总有一段数据无法发送到对端怎么办

10MB 的数据，TCP 传送。若其中协议封装的一段中，假如是 20 字节，总是发送失败，收不到 ack 回复，而其它数据都已经传送完毕，这个时候会发生什么？

## 服务端最多多少并发

https://mp.weixin.qq.com/s/zb8DdjVn-f1zc51evyZ_eg

注意，这是服务端，是一台机器上的最大并发。

如果改成一个服务进程，那最大应该是一个 int 的上限，因为 accept 的返回类型是 int，如下，

```
int accept(int sockfd, struct sockaddr *restrict addr,
                  socklen_t *restrict addrlen);
```

## epoll_ctl 和 epoll_wait 可以并行么

https://stackoverflow.com/questions/6847280/executing-epoll-ctl-in-one-thread-while-the-other-thread-is-in-the-middle-of-epo


## UDP 一个包最大可以发送多少个字节

- https://segmentfault.com/a/1190000017959319
- 

## IP 头部中的分片偏移量

https://juejin.cn/book/6844733788681928712/section/6844733788816179207， 当发送一个超长字节流时，链路层会进行分割，IP 头部中的分片偏移量就是用来进行还原的，那么为何不可以用序列号来还原呢？

因为 TCP 才有序列号，UDP 是没有的。
